{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunpandey2023/deep/blob/main/LangChain_Master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8tK3bJ9Etbg"
      },
      "source": [
        "#1 INSTALL ALL THE LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc0lU4IvWF1J"
      },
      "outputs": [],
      "source": [
        "pip install openai python-dotenv langchain pypdf yt_dlp Chroma chromadb kaleido python-multipart tiktoken lark pydub duckduckgo-search langchain[docarray] langchain_experimental langchain-cli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtR-TscUE2wF"
      },
      "source": [
        "#2 IMPORT ALL LIBRARIES I.E. OPENAI/LANGCHAIN/DUCKDUCKGO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlJ1B3haWIYv"
      },
      "outputs": [],
      "source": [
        "import os, openai, sys\n",
        "from langchain.document_loaders import PyPDFLoader,WebBaseLoader,TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.document_loaders.generic import GenericLoader\n",
        "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
        "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "import numpy as np\n",
        "import datetime\n",
        "import panel as pn\n",
        "import param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHLDtl45FB1x"
      },
      "source": [
        "#3 SET THE ENVIRONMENT AND OPENAI_API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQDvw_h2r54S"
      },
      "outputs": [],
      "source": [
        "llm_name = \"gpt-3.5-turbo\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API KEY\"\n",
        "#llm = OpenAI(temperature=0)\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "persist_directory = '/sample_data/chroma'\n",
        "sys.path.append('../..')\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=embedding\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TudD9H4FKok"
      },
      "source": [
        "#4 LOAD THE FILES YOU WANT TO USE FOR TESTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvGmYX33BaXd"
      },
      "source": [
        "##4.1 Using web Based loader to load files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9JPGIWu8lFY"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# List of URLs to load files from\n",
        "urls = [\n",
        "    \"https://docs.google.com/document/d/10D6fWeQu87YDeKLqOxjwh9eczaLTPEOWrxR9jNBV7ho/edit?usp=sharing\",\n",
        "    \"https://docs.google.com/document/d/1W9Gw6XwF5dVTzVEOHk-OJOJE9rrkPtC84bjt-gNvZNI/edit?usp=sharing\",\n",
        "    \"https://docs.google.com/document/d/1X6OyNNoAXHc4X0ryhu4x-J9cj-wntCER7OxN55_FWUo/edit?usp=sharing\",\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Initialize a list to store loaded documents\n",
        "loaded_documents = []\n",
        "\n",
        "# Create a WebBaseLoader instance for each URL and load the documents\n",
        "for url in urls:\n",
        "    loader = WebBaseLoader(url)\n",
        "    documents = loader.load()\n",
        "    loaded_documents.extend(documents)  # Append loaded documents to the list\n",
        "\n",
        "# Now `loaded_documents` contains the documents loaded from all URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FCKFUgbmz8p"
      },
      "outputs": [],
      "source": [
        "loaders = [\n",
        "    # Duplicate documents on purpose - messy data\n",
        "\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/c1.pdf\"),\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/c2.pdf\"),\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/c3.pdf\"),\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/c4.pdf\"),\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/c5.pdf\"),\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prXlgqzzFqt8"
      },
      "source": [
        "#5 SET THE SPLITTER AND VECTORDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYsyxEhImhpG"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B6OZyGwNvmT"
      },
      "outputs": [],
      "source": [
        "#vectordb.delete_collection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjhBtgm7F1sd"
      },
      "source": [
        "#6 CHECK THE DOCS AND VECTORDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHlwI3IbnHxH",
        "outputId": "4caf477c-0310-47bd-e267-95e92e030510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "print(len(docs))\n",
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXoyn0PYF-Xm"
      },
      "source": [
        "#7 START Q&A USING SIMILARITY SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqf9TnlMp5zF"
      },
      "outputs": [],
      "source": [
        "question = \"what is status of the application?\"\n",
        "docs = vectordb.similarity_search(question,k=3) # k is number of documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edJyq7cBqHRH"
      },
      "outputs": [],
      "source": [
        "docs[2].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DB8tk2AGS6a"
      },
      "source": [
        "#8 FIND THE DOCUMENTS USING COMPRESSION SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXHOjs1O2U8c"
      },
      "outputs": [],
      "source": [
        "def pretty_print_docs(docs):\n",
        "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm5UfpBR2Z3y"
      },
      "outputs": [],
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3_LTsf92b8Z"
      },
      "outputs": [],
      "source": [
        "question = \"what are the names of the counsels?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
        "pretty_print_docs(compressed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NlcMunZGfWc"
      },
      "source": [
        "#9 FIND THE DOCUMENTS USING MIXED MODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny6NKqTS3Zm1"
      },
      "outputs": [],
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcrSt15xHYJI"
      },
      "source": [
        "#10 RETRIEVE CONTENT USING RETRIEVAL QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3nJKeIV3cDm",
        "outputId": "8769b8b7-ab5a-432d-e51c-250baf7c3416"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 11, updating n_results = 11\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "\n",
            "the requirement of compulsory packaging of cement and\n",
            "fertilizer in jute bags has been done away with\n"
          ]
        }
      ],
      "source": [
        "question = \"what were the sections applied?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
        "pretty_print_docs(compressed_docs)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RuB6Qrci4Dei",
        "outputId": "6d30a1df-49f1-4353-9896-182eb010fe47"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The judges mentioned in the given context are:\\n1. HON'BLE MR. JUSTICE MANMOHAN\\n2. HON'BLE MR. JUSTICE SAURABH BANERJEE\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"find all the judges mentioned?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyfMD2hys-pI"
      },
      "source": [
        "#11 CHATBOT WHICH CAN WORK ON SELECTIVE DOCUMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDrrWcmbtUY8"
      },
      "outputs": [],
      "source": [
        "# This will initialize your database and retriever chain\n",
        "def load_db(file, chain_type, k):\n",
        "    # load documents\n",
        "    loader = PyPDFLoader(file)\n",
        "    documents = loader.load()\n",
        "    # split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    # define embedding\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    # create vector database from data\n",
        "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "    # define retriever\n",
        "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "    # create a chatbot chain. Memory is managed externally.\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=ChatOpenAI(model_name=llm_name, temperature=0),\n",
        "        chain_type=chain_type,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        return_generated_question=True,\n",
        "    )\n",
        "    return qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OxbUsq-te_N"
      },
      "outputs": [],
      "source": [
        "class cbfs(param.Parameterized):\n",
        "    chat_history = param.List([])\n",
        "    answer = param.String(\"\")\n",
        "    db_query  = param.String(\"\")\n",
        "    db_response = param.List([])\n",
        "\n",
        "    def __init__(self,  **params):\n",
        "        super(cbfs, self).__init__( **params)\n",
        "        self.panels = []\n",
        "        self.loaded_file = \"/content/drive/MyDrive/0_GENAIDATA/Introduction.pdf\"\n",
        "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
        "\n",
        "    def call_load_db(self, count):\n",
        "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
        "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "        else:\n",
        "            file_input.save(\"temp.pdf\")  # local copy\n",
        "            self.loaded_file = file_input.filename\n",
        "            button_load.button_style=\"outline\"\n",
        "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
        "            button_load.button_style=\"solid\"\n",
        "        self.clr_history()\n",
        "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
        "\n",
        "    def convchain(self, query):\n",
        "        if not query:\n",
        "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
        "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
        "        self.chat_history.extend([(query, result[\"answer\"])])\n",
        "        self.db_query = result[\"generated_question\"]\n",
        "        self.db_response = result[\"source_documents\"]\n",
        "        self.answer = result['answer']\n",
        "        self.panels.extend([\n",
        "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
        "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
        "        ])\n",
        "        inp.value = ''  #clears loading indicator when cleared\n",
        "        return pn.WidgetBox(*self.panels,scroll=True)\n",
        "\n",
        "    @param.depends('db_query ', )\n",
        "    def get_lquest(self):\n",
        "        if not self.db_query :\n",
        "            return pn.Column(\n",
        "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
        "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
        "            )\n",
        "        return pn.Column(\n",
        "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
        "            pn.pane.Str(self.db_query )\n",
        "        )\n",
        "\n",
        "    @param.depends('db_response', )\n",
        "    def get_sources(self):\n",
        "        if not self.db_response:\n",
        "            return\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
        "        for doc in self.db_response:\n",
        "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    @param.depends('convchain', 'clr_history')\n",
        "    def get_chats(self):\n",
        "        if not self.chat_history:\n",
        "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
        "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
        "        for exchange in self.chat_history:\n",
        "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
        "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
        "\n",
        "    def clr_history(self,count=0):\n",
        "        self.chat_history = []\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a-oG2J8uHX_"
      },
      "outputs": [],
      "source": [
        "cb = cbfs()\n",
        "\n",
        "file_input = pn.widgets.FileInput(accept='.pdf')\n",
        "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
        "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
        "button_clearhistory.on_click(cb.clr_history)\n",
        "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
        "\n",
        "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
        "conversation = pn.bind(cb.convchain, inp)\n",
        "\n",
        "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
        "\n",
        "tab1 = pn.Column(\n",
        "    pn.Row(inp),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab2= pn.Column(\n",
        "    pn.panel(cb.get_lquest),\n",
        "    pn.layout.Divider(),\n",
        "    pn.panel(cb.get_sources ),\n",
        ")\n",
        "tab3= pn.Column(\n",
        "    pn.panel(cb.get_chats),\n",
        "    pn.layout.Divider(),\n",
        ")\n",
        "tab4=pn.Column(\n",
        "    pn.Row( file_input, button_load, bound_button_load),\n",
        "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
        "    pn.layout.Divider(),\n",
        "    pn.Row(jpg_pane.clone(width=400))\n",
        ")\n",
        "dashboard = pn.Column(\n",
        "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
        "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
        ")\n",
        "pn.extension()\n",
        "dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpslcodrP6FG"
      },
      "source": [
        "#12 USING AGENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNqPSZtxlnzx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n",
        "agent = create_pandas_dataframe_agent(OpenAI(temperature=0), df, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "qsqMp8tBlvL9",
        "outputId": "512462d9-e66d-4794-ab4d-6c124f1d1923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find out how many rows are in the dataframe\n",
            "Action: python_repl_ast\n",
            "Action Input: len(df)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m3000\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: There are 3000 rows in the dataframe.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are 3000 rows in the dataframe.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.run(\"how many rows are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKpfPOS6nMfh",
        "outputId": "0f188a9d-a41c-4df9-8750-8af17507dfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3000, 9)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIas9aSuQVux"
      },
      "source": [
        "#13 GENERATE PYTHON CODE USING AGENTS AND EXECUTE THAT CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQvYSdCYnPpQ",
        "outputId": "b346b228-04af-4359-9050-41ce39806c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Name  Age      City\n",
            "0    John   25  New York\n",
            "1    Emma   28    London\n",
            "2    Mike   22     Paris\n",
            "3  Sophia   30     Tokyo\n",
            "\n"
          ]
        }
      ],
      "source": [
        "template = \"\"\"Write some python code to solve the user's problem.\n",
        "\n",
        "Return only python code in Markdown format, e.g.:\n",
        "\n",
        "```python\n",
        "....\n",
        "```\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "def _sanitize_output(text: str):\n",
        "    _, after = text.split(\"```python\")\n",
        "    return after.split(\"```\")[0]\n",
        "\n",
        "chain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run\n",
        "\n",
        "result = chain.invoke({\"input\": \"generate a python script to create a pandas dataframe\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOPK5WwNQgZ6"
      },
      "source": [
        "#14 USE DUCKDUCKGO TO GENERATE GENERIC SEARCHES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs7BmDEIo1MB",
        "outputId": "adf22952-45c5-487b-b425-ca5eb87b3eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deputy Prime Minister of United Kingdom Oliver Dowden addresses the 78th session of the United Nations General Assembly, Friday, Sept. 22, 2023, at United Nations headquarters. The prime minister of United Kingdom is the head of the government in the United Kingdom.12 May 1980 born Rishi Sunak held two Cabinet positions under Prime Minister Boris Johnson and has been a Member of Parliament (MP) for Richmond (Yorks) since 2015.Liz Truss left the office after the shortest-serving period of 49 days in history she just ...\n"
          ]
        }
      ],
      "source": [
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "template = \"\"\"Turn the following user input into a search query for a search engine:\n",
        "\n",
        "{input}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chain = prompt | model | StrOutputParser() | search\n",
        "\n",
        "search_result = chain.invoke({\"input\": \"who is the prime minister of united kingdom?\"})\n",
        "print(search_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZjj3z67V35P"
      },
      "source": [
        "#15 INPUT/OUTPUT IN DIFFERENT FORMATS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo7YDN_KpZR4",
        "outputId": "35a4a394-eb7b-4b35-e59e-5acab872f78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='As an AI, I do not have personal endorsements. However, Mahendra Singh Dhoni has been associated with several brands over the years, including Pepsi, Reebok, Sony, Gulf Oil, GoDaddy, and many more. Please note that endorsement deals may change over time.'\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema.messages import HumanMessage, SystemMessage\n",
        "chat = ChatOpenAI()\n",
        "messages = [\n",
        "    SystemMessage(content=\"you are the M S Dhoni\"),\n",
        "    HumanMessage(content=\"which brands do you endorse\"),\n",
        "]\n",
        "response = chat.invoke(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCYsvYtwYHti",
        "outputId": "2f3e9493-fc0b-4a35-a6eb-56f2a8ebbb20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setup='Why did the chicken cross the road?' punchline='To get to the other side!'\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "\n",
        "# Initialize the language model\n",
        "model = OpenAI(model_name=\"text-davinci-003\", temperature=0.0)\n",
        "\n",
        "# Define your desired data structure using Pydantic\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "    @validator(\"setup\")\n",
        "    def question_ends_with_question_mark(cls, field):\n",
        "        if field[-1] != \"?\":\n",
        "            raise ValueError(\"Badly formed question!\")\n",
        "        return field\n",
        "\n",
        "# Set up a PydanticOutputParser\n",
        "parser = PydanticOutputParser(pydantic_object=Joke)\n",
        "\n",
        "# Create a prompt with format instructions\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Define a query to prompt the language model\n",
        "query = \"Tell me a joke.\"\n",
        "\n",
        "# Combine prompt, model, and parser to get structured output\n",
        "prompt_and_model = prompt | model\n",
        "output = prompt_and_model.invoke({\"query\": query})\n",
        "\n",
        "# Parse the output using the parser\n",
        "parsed_result = parser.invoke(output)\n",
        "\n",
        "# The result is a structured object\n",
        "print(parsed_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUaoUVTxb2k4",
        "outputId": "240312a2-41d1-4ef2-8336-3f6e6da752cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{}, {'birthdate': ''}, {'birthdate': 'June'}, {'birthdate': 'June 28'}, {'birthdate': 'June 28,'}, {'birthdate': 'June 28, 1971'}, {'birthdate': 'June 28, 1971', 'birthplace': ''}, {'birthdate': 'June 28, 1971', 'birthplace': 'P'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pret'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria,'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria, South'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria, South Africa'}]\n"
          ]
        }
      ],
      "source": [
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "# Create a JSON prompt\n",
        "json_prompt = PromptTemplate.from_template(\n",
        "    \"Return a JSON object with `birthdate` and `birthplace` key that answers the following question: {question}\"\n",
        ")\n",
        "\n",
        "# Initialize the JSON parser\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "# Create a chain with the prompt, model, and parser\n",
        "json_chain = json_prompt | model | json_parser\n",
        "\n",
        "# Stream through the results\n",
        "result_list = list(json_chain.stream({\"question\": \"When and where was Elon Musk born?\"}))\n",
        "\n",
        "# The result is a list of JSON-like dictionaries\n",
        "print(result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SzAS-p_dSVh",
        "outputId": "7d5171f9-78d2-4772-bd08-cf2b97088761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Virat Kohli', 'Rohit Sharma', 'Shikhar Dhawan', 'Jasprit Bumrah', 'Ravindra Jadeja']\n"
          ]
        }
      ],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize the parser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# Create format instructions\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Create a prompt to request a list\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "# Define a query to prompt the model\n",
        "query = \"Indian Cricket Team\"\n",
        "\n",
        "# Generate the output\n",
        "output = model(prompt.format(subject=query))\n",
        "\n",
        "# Parse the output using the parser\n",
        "parsed_result = output_parser.parse(output)\n",
        "\n",
        "# The result is a list of items\n",
        "print(parsed_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYI8rjXNdjF3",
        "outputId": "2f9ef3c8-15f4-4003-b633-6eb8cd3a30a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1969-07-20 20:17:40\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize the DatetimeOutputParser\n",
        "output_parser = DatetimeOutputParser()\n",
        "\n",
        "# Create a prompt with format instructions\n",
        "template = \"\"\"\n",
        "Answer the user's question:\n",
        "{question}\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template,\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Create a chain with the prompt and language model\n",
        "chain = LLMChain(prompt=prompt, llm=OpenAI())\n",
        "\n",
        "# Define a query to prompt the model\n",
        "query = \"when did Neil Armstrong land on the moon in terms of GMT?\"\n",
        "\n",
        "# Run the chain\n",
        "output = chain.run(query)\n",
        "\n",
        "# Parse the output using the datetime parser\n",
        "parsed_result = output_parser.parse(output)\n",
        "\n",
        "# The result is a datetime object\n",
        "print(parsed_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfFPTS8d9qP"
      },
      "source": [
        "#16 RETRIEVAL IN LANGCHAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vmcZt8cdt7g"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"/content/drive/MyDrive/1_Gen_AI.txt\")\n",
        "document = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2w9dJjUeSV3"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path='/content/sample_data/california_housing_test.csv')\n",
        "documents = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "G2QSlDHYemDT",
        "outputId": "b51ce24b-eb09-4057-c680-ff93cb17f822"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nloader = CSVLoader(file_path=\\'./example_data/mlb_teams_2012.csv\\', csv_args={\\n    \\'delimiter\\': \\',\\',\\n    \\'quotechar\\': \\'\"\\',\\n    \\'fieldnames\\': [\\'MLB Team\\', \\'Payroll in millions\\', \\'Wins\\']\\n})\\ndocuments = loader.load()\\n'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv', csv_args={\n",
        "    'delimiter': ',',\n",
        "    'quotechar': '\"',\n",
        "    'fieldnames': ['MLB Team', 'Payroll in millions', 'Wins']\n",
        "})\n",
        "documents = loader.load()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wqLNF6Qew2m"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/0_GENAIDATA/Introduction Yottaasys 15_October.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WB3BO0OWe66d",
        "outputId": "beecb36e-31a5-454c-9f43-29c1585ff355"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from langchain.document_loaders import MathpixPDFLoader\\nloader = MathpixPDFLoader(\"/content/drive/MyDrive/0_GENAIDATA/Introduction Yottaasys 15_October.pdf\")\\ndata = loader.load()\\n'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from langchain.document_loaders import MathpixPDFLoader\n",
        "loader = MathpixPDFLoader(\"/content/drive/MyDrive/0_GENAIDATA/Introduction Yottaasys 15_October.pdf\")\n",
        "data = loader.load()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6AYCLAVfnLM"
      },
      "outputs": [],
      "source": [
        "pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOlvO2qVfDsV",
        "outputId": "716fc278-1db6-4965-cea4-68e71de43127"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py:Received runtime arguments {'option': 'text'}. Passing runtime args to `load` is deprecated. Please pass arguments during initialization instead.\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "loader = PyMuPDFLoader(\"/content/drive/MyDrive/0_GENAIDATA/Introduction Yottaasys 15_October.pdf\")\n",
        "data = loader.load()\n",
        "# Optionally pass additional arguments for PyMuPDF's get_text() call\n",
        "data = loader.load(option=\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOf5qjzAf57D"
      },
      "outputs": [],
      "source": [
        "pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTNj12cffV7w"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PDFMinerLoader\n",
        "\n",
        "loader = PDFMinerLoader(\"/content/drive/MyDrive/0_GENAIDATA/Introduction Yottaasys 15_October.pdf\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQL0UrY3gM9r"
      },
      "outputs": [],
      "source": [
        "pip install amazon-textract-caller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKeTpF05f1GP"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import AmazonTextractPDFLoader\n",
        "\n",
        "# Requires AWS account and configuration\n",
        "loader = AmazonTextractPDFLoader(\"/content/drive/MyDrive/0_GENAIDATA/Introduction Yottaasys 15_October.pdf\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVNYTZFscSUE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRcFnws-Vo1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "882e1cc0"
      },
      "source": [
        "## ✅ 17 (Enhanced) - Load Web Content using Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8c65463"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "\n",
        "# Load documents related to Artificial Intelligence\n",
        "loader = WikipediaLoader(query=\"Artificial Intelligence\", load_max_docs=2)\n",
        "wiki_docs = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(wiki_docs)} documents\")\n",
        "print(wiki_docs[0].page_content[:500])  # Preview content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5130795d"
      },
      "source": [
        "## ✅ 18 (Enhanced) - Chatbot with Memory and Specific Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08999686"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Assume `vectorstore` and `retriever` were created previously\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,  # Replace with your chosen LLM\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "query = \"What is the main advantage of AI in healthcare?\"\n",
        "result = qa_chain.run(query)\n",
        "print(result)\n",
        "\n",
        "# Follow-up question to test memory\n",
        "followup = \"Can you give an example of that?\"\n",
        "print(qa_chain.run(followup))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02a4b66"
      },
      "source": [
        "## ✅ 19 (Enhanced) - Use Agent to Run Python Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc240b73"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "# Load Python REPL tool\n",
        "tools = load_tools([\"python_repl_tool\"], llm=llm)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "response = agent.run(\"What is the square root of 256 plus 10?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32d0ae0a"
      },
      "source": [
        "## ✅ 20 Load CSV File into LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6339552e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "\n",
        "# Assume you have a CSV with a 'content' column\n",
        "csv_loader = CSVLoader(file_path=\"sample.csv\", source_column=\"content\")\n",
        "csv_docs = csv_loader.load()\n",
        "\n",
        "print(f\"Loaded {len(csv_docs)} CSV documents\")\n",
        "print(csv_docs[0].page_content[:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f25a47e"
      },
      "source": [
        "## ✅ 21 Use Multiple Retrievers and Combine Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f20f996d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "# Assume two vectorstores: vs1 and vs2\n",
        "retriever1 = vs1.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever2 = vs2.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "ensemble = EnsembleRetriever(retrievers=[retriever1, retriever2], weights=[0.5, 0.5])\n",
        "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, retriever=ensemble)\n",
        "\n",
        "response = qa_chain(\"What are the applications of AI in education?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e3e06f5"
      },
      "source": [
        "## ✅ 22 Evaluate QA Performance with Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55dae9b3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.evaluation.qa import QAEvalChain\n",
        "\n",
        "examples = [\n",
        "    {\"query\": \"What is AI?\", \"answer\": \"AI is the simulation of human intelligence by machines.\"}\n",
        "]\n",
        "\n",
        "predictions = [\n",
        "    {\"query\": \"What is AI?\", \"result\": \"AI is a branch of computer science that deals with intelligence simulation.\"}\n",
        "]\n",
        "\n",
        "eval_chain = QAEvalChain.from_llm(llm)\n",
        "graded_outputs = eval_chain.evaluate(examples, predictions)\n",
        "\n",
        "for result in graded_outputs:\n",
        "    print(result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}